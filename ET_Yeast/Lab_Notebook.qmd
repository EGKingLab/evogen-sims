---
title: "Lab_Notebook"
author: "Esdras T"
format: html
editor: visual
---

## Simulation project

Here are the notes taken while working on the yeast simulation model. Most of these notes are based on code writing and some additional ideas that might come up while working on the project.

## Writing Flow

The most current notes are at the top.
[** Wow!! April 28th 2023\***{.underline}

[I need to discuss with Libby how we can use mean and standard deviation while setting the optimum. I have realized that depending on the number of loci, the mean phenotype across replicates can vary, with more loci having a high error rate (mean being away from zero after adjustment). Therefore, we can still set the optimum to be based on the new mean and standard deviation so that the selection pressure can be the same across different number of loci. This is indicated by how all loci can have same fitness value at at given phenotype.]{.underline}


[**November 5th 2023\***]{.underline}

[How about you check how mean fitness changes after the shift in optimum? This can be achieved by comparing values at 25 and 26th generation , for example]{.underline}

[**November 5th 2023\***]{.underline}

[What are the characteristics of loci that are responding to the temporal variation and how how long do those changes last. This will have something to do with the initial allele frequency (or simply frequency and Fitness)]{.underline}.

[**September 26th 2023**]{.underline}

1.  The initial shift in optimum matters (+/-)
2.  If you use an **`early()`** callback, the setup operations would be performed at the beginning of the first generation, before any other events (like recombination or mutation) occur. If you use a **`late()`** callback, these operations would be performed at the end of the first generation, after all other events have occurred.

[**August 30th 2023**]{.underline}

With 1/P in our fitness function, we're rewarding individuals with a high phenotype despite that both high and low phenotype may receive a penalty based on the distance from optimum.

The question right now is to know how much penalty and how much reward are individuals getting. - The sure thing is that the difference between those individuals' fitness will be predicated on how many standard deviations is optimum from the mean phenotype. The larger the standard deviation, the more rewards for an individual with large phenotype. Hence, this must be directional selection.

Below is the code for demonstration.

```{r}
myexp <- function(P, Popt){
  w = exp(-(1/mean(P))*(P-Popt)^2)
  return(w)
}

P <- rnorm(20, 20, 2) %>% sort() 
mean(P)
sd(P)

Popt <- mean(P)+2*sd(P)

m <- abs(Popt - P)
mm = m %>% as.numeric() %>% sort()

W <- sapply(P, myexp, Popt = Popt)

W
```

[**July 6th 2023**]{.underline}

With bash, we can run our model with minimal lines while using for loop to work on every parameter. For parameters line number of loci and generation interval, which need other variables to maintain a single genome size, we must create keys that are associated with those values.

For example:

> declare -A loci_to_regions
>
> loci_to_regions=( \[1\]=610140 \[10\]=61014 \[70\]=8716 \[100\]=6101 \[300\]=2033 )
>
> declare -A generations_to_ranges
>
> generations_to_ranges=( \[10\]=101 \[20\]=51 \[30\]=34 )

Then

> for loci in "\${!loci_to_regions\[\@\]}"
>
> do
>
> for gen in "\${!generations_to_ranges\[\@\]}"
>
> do
>
> region=\${loci_to_regions\[\$loci\]}
>
> rang=\${generations_to_ranges\[\$gen\]}

[***June 14th 2023***]{.underline}

I worked on the change in phenotypic optimum which follows sinusoidal function. The optimum is constrained in some standard deviations from the mean.

Also, I tried the changing optimum which change of generation interval. The code I used resulted in more than the expected (4) peaks from spectral analysis. This might be due to so many factors including:

-   Noises

-   Consecutive signals of different periods which can lead to non-stationary behavior (then wavelet would be desirable)

<!-- -->

-   Repetition of the signals for 29 times (Look the code below)

    ```{r}
    t = 1
    generations = seq(1, 2001,1)
    Y_hat = c();
    for (i in c(30, 15, 10, 20)) {
     y = stdv*sd(phenotypes) * sin(2 * (22/7) * generations[t:i] / i) + mean(phenotypes)
     t = t + i
     Y_hat = c(Y_hat, y)
      }
    nopt = rep(Y_hat, 29)
    ```

To correct that, I am going to try the following code as it has been tested in R

```{r}
generations <- seq(1, 2001,1);

nopt <- rep(0, length(generations));

for (i in c(30, 15, 10, 20)) {
  nopt <- nopt + stdv*sd(phenotypes) * sin(2 * (22/7) * generations / i) + 100;
}
```

[***June 8th 2023***]{.underline}

Now that I have modeled Selection with changing generations & optimum based on standard deviations, I should now think of modeling irregular periodicity (how many generations the optimum should change) or how much the environment should affect the optimum (1sd, 2sd, or even a different number following a given distribution).

[***June 5th 2023***]{.underline}

Running constant and fluctuating selection while considering 100 loci, different selection strength (Optimum having some standard deviations from mean phenotypes), and several levels of intervals.

Standard deviation: 1, 2, 3, and 4

Levels of interval: 10, 20, and 30.

The names of output files having r*eplicate number, loci numbers, heritability value, standard deviations, and generation intervals*. For example: genome5_100_H0.5SD2Gen20.csv

[***June 4th 2023***]{.underline}

Here are some of the fitness functions to try. The 3rd and 4th functions are pretty much the same and they work very well for scaled phenotype. They are also considered as a strong selection while the 2nd function is a weak selection. The 5th function work with non-scaled phenotypes. The first and last functions might need to be modified.

1.  inds.fitnessScaling = 1 + dnorm(newopt-phenotypes,0,0.25);
2.  inds.fitnessScaling = 1-(abs(phenotypes - newopt\[sim.cycle\])/200);
3.  inds.fitnessScaling = exp(-(0.02) \* (phenotypes - newopt\[sim.cycle\])\^2);
4.  **inds.fitnessScaling = exp(-(1/(phenotypes)) \* (phenotypes - newopt\[sim.cycle\])\^2);**
5.  inds.fitnessScaling = exp(-(1/mean(phenotypes)\^2) \* (phenotypes - newopt\[sim.cycle\])\^2); //# This fitness goes with unscaled phenotype
6.  1.0 + dnorm(100 - phenotypes, 0.0, 5.0);

[***May 30th 2023***]{.underline}

The peak frequency can indicate where there is the highest power (spectral density), and that would lead us to how many cycles (signal) are in our dataset (1/peak frequency). The total number of periods divided by the number of cycles will tell us when the signal comes back.

Note that MCMC sampling can be used to validate our peaks. (Review your notes)

[***May 23rd 2023***]{.underline}

Have been working with FFT. I have figured out that the best way to work with them for each QTL position is to use a for loop or a function (Check your current code). While I am still processing data on every QTL along all generations, it would instead be feasible to do the analysis and then find the mean Frequency for each generation. From there, we can work on the plots. Despite everything that I have done so far, I need to find the best way to post-processing for Fourier. Also, I need to check if all Fourier assumptions are met otherwise I would consider wavelet assumptions. One of the assumptions of Fourier is "The sampled signal segment must contain a whole number of [periods](https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft#:~:text=In%20the%20Fourier%20transformation%2C%20the%20assumption%20is%20that,segment%20must%20contain%20a%20whole%20number%20of%20periods.). (Does this means that a QTL has to be present for all generations?????)"

[***March 6th 2023***]{.underline}

Started the project while following a single step per time. The first stem was to set the genome size and the number of QTLs. The genome size will approximately equal to 1,220,281 (arbitrary), and the number of QTLs considered are 1 with interval (I) of 610,140, 10 QTLs (I = 110,934), 70 QTLs (I = 17,186), 100 QTLs (I = 12,081), 300 QTLs (I = 4,053).

> To write the file, the file name will include the replicate, QTL#, and heritability value

On the plan, there is writing a bash code that combines all the files together.

[***February 20th 2023***]{.underline}

Remember that to output the whole genome file, you can use sim.outputFull("path to your directory"). The same applies when you want to save VCF file and some other important files.

[***January 7th 2023***]{.underline}

\[***defineGlobal***\] makes your object available for use. For example, the new optimum or new population size. ?? How is it different from define constant? For a constant, it is mostly a single number while Global can be a list.

[***January 5th 2023***]{.underline}

The tick and cycle events in Slim make sense especially when we are considering overlapping populations. That is, one person might have a child, grand-child, and grand grandkids. Therefore, for that particular person, we might see one tick, and have multiple cycles. \[That's how I would explain the tick and cycle thing\]

[***December 23rd 2022***]{.underline}

I today figured out that .tagF wouldn't work until we scale the fitness. Maybe it might be good to read more about it in the Eidos manual. Also, according to the plots, it seemed like the mutation doesn't change much about the frequency over generation when using a genome from a dummy population

[***December 5th 2022***]{.underline}

Creating an initial genome using a dummy population. In this case, we need a sub-population with a single genome that has added mutations and its effect size corresponds to the number of QTLs.

```{r, echo=FALSE}
library(slimr)
slim_block(1, early(),
             {sim.addSubpop("p2", 1);
	g = p2.genomes[0];
	muts = g.addNewMutation(m2, rexp(100), Q);
	
	sim.addSubpop("p1", 10000);
	marks = runif(100, 0,1);
	ind = 0;
	for(qpos in Q)
	{
		for(g in p1.genomes)
		{
			if(rbinom(1,1,marks[ind])==1){g.addMutations(muts[ind]);}
		}
		ind = ind + 1;
	}
	p2.setSubpopulationSize(0);})

```

Have an initial frequency distribution; draw random positions where your mutations could be in the sub-population to use.

[***November 29th 2022***]{.underline}

[*The very beginning of the notes.*]{.underline}

During the one on one meeting, we discussed about selection coefficient. I came to realize that our selection is predicated on the **optimum phenotype** not the selection coefficient we set. For fluctuating selection, we will be **changing the optimum phenotype at a given generation interval**.

Also, we talked about the introduction of new mutations. When only one individual carries the mutation (assuming beneficial mutation), that individual will have a high fitness thus higher reproductive priorities. Therefore, the mutation will fix so quickly.
